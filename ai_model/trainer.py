"""
Entra√Æneur pour le mod√®le LSTM de pr√©diction Volatility 10
G√®re l'entra√Ænement, la validation et l'optimisation des hyperparam√®tres
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import logging
import joblib
import os
import json
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
import warnings
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, precision_score, recall_score, \
    f1_score
import matplotlib.pyplot as plt
import seaborn as sns

from config import config
from data import prepare_training_data, FeatureSet
from .lstm_model import LSTMModel, ModelConfig, ModelMetrics, ModelArchitecture, PredictionType

# Configuration du logger
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')


@dataclass
class TrainingConfig:
    """Configuration pour l'entra√Ænement"""
    # Donn√©es
    symbol: str = "R_10"
    timeframe: str = "1m"
    training_days: int = 30
    validation_split: float = 0.2
    test_split: float = 0.1

    # Entra√Ænement
    batch_size: int = 32
    epochs: int = 100
    patience: int = 15
    min_delta: float = 0.001

    # Validation
    cross_validation_folds: int = 5
    validation_method: str = "time_series"  # "time_series" ou "random"

    # Sauvegarde
    model_dir: str = "models"
    save_best_only: bool = True
    save_history: bool = True

    # Optimisation
    auto_hyperparameter_tuning: bool = False
    hyperparameter_trials: int = 50

    # Performance
    min_accuracy_threshold: float = 0.55
    min_sharpe_threshold: float = 1.0
    retrain_threshold: float = 0.05  # D√©clin de performance pour red√©clencher l'entra√Ænement


@dataclass
class TrainingResults:
    """R√©sultats d'entra√Ænement"""
    # M√©triques finales
    final_metrics: ModelMetrics
    best_metrics: ModelMetrics

    # Historique
    training_history: Dict[str, List[float]]
    validation_history: Dict[str, List[float]]

    # Informations
    training_time: float
    epochs_completed: int
    best_epoch: int
    convergence_achieved: bool

    # Chemins
    model_path: str
    weights_path: str
    history_path: str

    def to_dict(self) -> Dict[str, Any]:
        return {
            'final_metrics': asdict(self.final_metrics),
            'best_metrics': asdict(self.best_metrics),
            'training_time': self.training_time,
            'epochs_completed': self.epochs_completed,
            'best_epoch': self.best_epoch,
            'convergence_achieved': self.convergence_achieved,
            'model_path': self.model_path,
            'weights_path': self.weights_path,
            'history_path': self.history_path
        }


class ModelTrainer:
    """Entra√Æneur pour les mod√®les LSTM"""

    def __init__(self, training_config: TrainingConfig = None):
        self.config = training_config or TrainingConfig()
        self.model = None
        self.training_results = None

        # Historique des entra√Ænements
        self.training_history = []

        # M√©triques de suivi
        self.training_stats = {
            'models_trained': 0,
            'total_training_time': 0.0,
            'best_accuracy_achieved': 0.0,
            'best_sharpe_achieved': 0.0,
            'last_training_date': None,
            'convergence_rate': 0.0
        }

        # Cr√©er les dossiers n√©cessaires
        self._create_directories()

        logger.info("Entra√Æneur de mod√®les LSTM initialis√©")

    def _create_directories(self):
        """Cr√©e les dossiers n√©cessaires pour la sauvegarde"""
        directories = [
            self.config.model_dir,
            os.path.join(self.config.model_dir, "weights"),
            os.path.join(self.config.model_dir, "history"),
            os.path.join(self.config.model_dir, "plots"),
            "logs/training"
        ]

        for directory in directories:
            os.makedirs(directory, exist_ok=True)

    def prepare_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Pr√©pare les donn√©es d'entra√Ænement, validation et test

        Returns:
            Tuple (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        try:
            logger.info(f"Pr√©paration des donn√©es pour {self.config.symbol}")

            # R√©cup√©rer les donn√©es
            end_date = datetime.now(timezone.utc)
            start_date = end_date - timedelta(days=self.config.training_days)

            feature_set = prepare_training_data(
                symbol=self.config.symbol,
                timeframe=self.config.timeframe,
                start_date=start_date,
                end_date=end_date
            )

            if feature_set.price_features.empty:
                raise ValueError("Aucune donn√©e disponible pour l'entra√Ænement")

            # Pr√©parer les s√©quences avec le mod√®le
            if self.model is None:
                # Cr√©er un mod√®le temporaire pour la pr√©paration des donn√©es
                temp_model = LSTMModel()
                X, y = temp_model.prepare_sequences(feature_set)
            else:
                X, y = self.model.prepare_sequences(feature_set)

            # Division des donn√©es
            total_samples = len(X)
            test_size = int(total_samples * self.config.test_split)
            val_size = int(total_samples * self.config.validation_split)
            train_size = total_samples - test_size - val_size

            # Division temporelle (importante pour les s√©ries temporelles)
            X_train = X[:train_size]
            y_train = y[:train_size]

            X_val = X[train_size:train_size + val_size]
            y_val = y[train_size:train_size + val_size]

            X_test = X[train_size + val_size:]
            y_test = y[train_size + val_size:]

            logger.info(f"Donn√©es pr√©par√©es: {train_size} train, {val_size} val, {test_size} test")
            logger.info(f"Forme des donn√©es: X={X_train.shape}, y={y_train.shape}")

            return X_train, X_val, X_test, y_train, y_val, y_test

        except Exception as e:
            logger.error(f"Erreur pr√©paration des donn√©es: {e}")
            raise

    def train_model(self, model_config: ModelConfig = None, X_train: np.ndarray = None,
                    y_train: np.ndarray = None, X_val: np.ndarray = None,
                    y_val: np.ndarray = None) -> TrainingResults:
        """
        Entra√Æne un mod√®le LSTM

        Args:
            model_config: Configuration du mod√®le
            X_train, y_train: Donn√©es d'entra√Ænement (optionnel, sinon pr√©par√©es automatiquement)
            X_val, y_val: Donn√©es de validation (optionnel)

        Returns:
            R√©sultats d'entra√Ænement
        """
        try:
            start_time = datetime.now(timezone.utc)
            logger.info("üöÄ D√©but de l'entra√Ænement du mod√®le LSTM")

            # Pr√©parer les donn√©es si non fournies
            if X_train is None:
                X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_data()

            # Cr√©er le mod√®le
            if model_config is None:
                model_config = ModelConfig()

            self.model = LSTMModel(model_config)

            # D√©finir la forme d'entr√©e
            input_shape = (X_train.shape[1], X_train.shape[2])
            keras_model = self.model.create_model(input_shape)

            # Chemins de sauvegarde
            timestamp = start_time.strftime("%Y%m%d_%H%M%S")
            model_name = f"lstm_{model_config.architecture.value}_{timestamp}"

            model_path = os.path.join(self.config.model_dir, f"{model_name}.h5")
            weights_path = os.path.join(self.config.model_dir, "weights", f"{model_name}_weights.h5")
            history_path = os.path.join(self.config.model_dir, "history", f"{model_name}_history.json")

            # Callbacks
            callbacks = self._create_callbacks(model_path, weights_path)

            # Entra√Ænement
            logger.info(f"Entra√Ænement avec {len(X_train)} √©chantillons")

            history = keras_model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                batch_size=self.config.batch_size,
                epochs=self.config.epochs,
                callbacks=callbacks,
                verbose=1,
                shuffle=False  # Important pour les s√©ries temporelles
            )

            # Sauvegarder le mod√®le final
            self.model.model = keras_model

            # Calculer les m√©triques finales
            final_metrics = self._calculate_metrics(keras_model, X_val, y_val, "validation")

            # Trouver la meilleure √©poque
            best_epoch = self._find_best_epoch(history)
            best_metrics = self._extract_best_metrics(history, best_epoch)

            # Temps d'entra√Ænement
            end_time = datetime.now(timezone.utc)
            training_time = (end_time - start_time).total_seconds()

            # Cr√©er les r√©sultats
            training_results = TrainingResults(
                final_metrics=final_metrics,
                best_metrics=best_metrics,
                training_history=history.history,
                validation_history=history.history,
                training_time=training_time,
                epochs_completed=len(history.history['loss']),
                best_epoch=best_epoch,
                convergence_achieved=self._check_convergence(history),
                model_path=model_path,
                weights_path=weights_path,
                history_path=history_path
            )

            # Sauvegarder les r√©sultats
            self._save_training_results(training_results, history_path)

            # Mettre √† jour les statistiques
            self._update_training_stats(training_results)

            # G√©n√©rer les graphiques
            self._plot_training_history(history, model_name)

            self.training_results = training_results

            logger.info(f"‚úÖ Entra√Ænement termin√© en {training_time:.1f}s")
            logger.info(f"üìä Pr√©cision finale: {final_metrics.accuracy:.3f}")
            logger.info(f"üìà Meilleure pr√©cision: {best_metrics.accuracy:.3f} (√©poque {best_epoch})")

            return training_results

        except Exception as e:
            logger.error(f"Erreur lors de l'entra√Ænement: {e}")
            raise

    def _create_callbacks(self, model_path: str, weights_path: str) -> List:
        """Cr√©e les callbacks pour l'entra√Ænement"""
        callbacks = []

        # Early stopping
        early_stopping = keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=self.config.patience,
            min_delta=self.config.min_delta,
            restore_best_weights=True,
            verbose=1
        )
        callbacks.append(early_stopping)

        # R√©duction du learning rate
        reduce_lr = keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=max(5, self.config.patience // 3),
            min_lr=1e-7,
            verbose=1
        )
        callbacks.append(reduce_lr)

        # Sauvegarde du mod√®le
        if self.config.save_best_only:
            checkpoint = keras.callbacks.ModelCheckpoint(
                filepath=model_path,
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=False,
                verbose=1
            )
            callbacks.append(checkpoint)

            # Sauvegarde des weights s√©par√©ment
            weights_checkpoint = keras.callbacks.ModelCheckpoint(
                filepath=weights_path,
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=True,
                verbose=0
            )
            callbacks.append(weights_checkpoint)

        # Logging personnalis√©
        class TrainingLogger(keras.callbacks.Callback):
            def __init__(self, logger):
                super().__init__()
                self.logger = logger
                self.epoch_start_time = None

            def on_epoch_begin(self, epoch, logs=None):
                self.epoch_start_time = datetime.now()

            def on_epoch_end(self, epoch, logs=None):
                duration = (datetime.now() - self.epoch_start_time).total_seconds()
                if (epoch + 1) % 10 == 0:
                    self.logger.info(f"√âpoque {epoch + 1}: loss={logs.get('loss', 0):.4f}, "
                                     f"val_loss={logs.get('val_loss', 0):.4f}, "
                                     f"dur√©e={duration:.1f}s")

        callbacks.append(TrainingLogger(logger))

        return callbacks

    def _calculate_metrics(self, model: keras.Model, X: np.ndarray, y: np.ndarray, prefix: str) -> ModelMetrics:
        """Calcule les m√©triques de performance"""
        try:
            metrics = ModelMetrics()

            # Pr√©dictions
            y_pred = model.predict(X, verbose=0)

            # Selon le type de pr√©diction
            if isinstance(y_pred, list):  # Multi-output
                y_pred_price = y_pred[0].flatten()
                y_pred_direction = y_pred[1]
                y_pred_confidence = y_pred[2].flatten()

                # M√©triques de r√©gression (prix)
                y_true_price = y[:, 0] if len(y.shape) > 1 else y
                metrics.mae = mean_absolute_error(y_true_price, y_pred_price)
                metrics.rmse = np.sqrt(mean_squared_error(y_true_price, y_pred_price))
                metrics.mape = np.mean(np.abs((y_true_price - y_pred_price) / (y_true_price + 1e-8))) * 100

                # M√©triques de classification (direction)
                if len(y.shape) > 1 and y.shape[1] > 1:
                    y_true_direction = y[:, 1:4]  # Classes de direction
                    y_pred_direction_classes = np.argmax(y_pred_direction, axis=1)
                    y_true_direction_classes = np.argmax(y_true_direction, axis=1)

                    metrics.accuracy = accuracy_score(y_true_direction_classes, y_pred_direction_classes)
                    metrics.precision = precision_score(y_true_direction_classes, y_pred_direction_classes,
                                                        average='weighted', zero_division=0)
                    metrics.recall = recall_score(y_true_direction_classes, y_pred_direction_classes,
                                                  average='weighted', zero_division=0)
                    metrics.f1_score = f1_score(y_true_direction_classes, y_pred_direction_classes, average='weighted',
                                                zero_division=0)

                # Pr√©cision directionnelle
                metrics.directional_accuracy = self._calculate_directional_accuracy(y_true_price, y_pred_price)

            else:  # Single output
                if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:
                    # Classification
                    y_pred_classes = np.argmax(y_pred, axis=1)
                    y_true_classes = np.argmax(y, axis=1) if len(y.shape) > 1 else y.astype(int)

                    metrics.accuracy = accuracy_score(y_true_classes, y_pred_classes)
                    metrics.precision = precision_score(y_true_classes, y_pred_classes, average='weighted',
                                                        zero_division=0)
                    metrics.recall = recall_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)
                    metrics.f1_score = f1_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)
                else:
                    # R√©gression
                    y_pred_flat = y_pred.flatten()
                    y_true_flat = y.flatten()

                    metrics.mae = mean_absolute_error(y_true_flat, y_pred_flat)
                    metrics.rmse = np.sqrt(mean_squared_error(y_true_flat, y_pred_flat))
                    metrics.mape = np.mean(np.abs((y_true_flat - y_pred_flat) / (y_true_flat + 1e-8))) * 100

                    # Pr√©cision directionnelle
                    metrics.directional_accuracy = self._calculate_directional_accuracy(y_true_flat, y_pred_flat)

            # M√©triques financi√®res
            if metrics.directional_accuracy > 0:
                # Sharpe ratio simplifi√© (bas√© sur la pr√©cision directionnelle)
                excess_return = (metrics.directional_accuracy - 0.5) * 2  # Convertir en rendement exc√©dentaire
                metrics.sharpe_ratio = excess_return / max(0.1, metrics.rmse / 100)  # Approximation

            logger.debug(f"M√©triques {prefix} calcul√©es: accuracy={metrics.accuracy:.3f}, "
                         f"directional_accuracy={metrics.directional_accuracy:.3f}")

            return metrics

        except Exception as e:
            logger.error(f"Erreur calcul des m√©triques: {e}")
            return ModelMetrics()

    def _calculate_directional_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """Calcule la pr√©cision directionnelle"""
        try:
            if len(y_true) < 2:
                return 0.0

            # Calculer les changements directionnels
            true_direction = np.diff(y_true) > 0
            pred_direction = np.diff(y_pred) > 0

            # Pr√©cision directionnelle
            correct_directions = np.sum(true_direction == pred_direction)
            total_predictions = len(true_direction)

            return correct_directions / total_predictions if total_predictions > 0 else 0.0

        except Exception as e:
            logger.error(f"Erreur calcul pr√©cision directionnelle: {e}")
            return 0.0

    def _find_best_epoch(self, history) -> int:
        """Trouve la meilleure √©poque bas√©e sur val_loss"""
        val_loss = history.history.get('val_loss', [])
        if val_loss:
            return int(np.argmin(val_loss))
        return 0

    def _extract_best_metrics(self, history, best_epoch: int) -> ModelMetrics:
        """Extrait les m√©triques de la meilleure √©poque"""
        metrics = ModelMetrics()

        if best_epoch < len(history.history.get('loss', [])):
            metrics.val_loss = history.history.get('val_loss', [0])[best_epoch]

            # Accuracy si disponible
            if 'val_accuracy' in history.history:
                metrics.val_accuracy = history.history['val_accuracy'][best_epoch]
            elif 'val_direction_prediction_accuracy' in history.history:
                metrics.val_accuracy = history.history['val_direction_prediction_accuracy'][best_epoch]

            # MAE si disponible
            if 'val_mae' in history.history:
                metrics.mae = history.history['val_mae'][best_epoch]
            elif 'val_price_prediction_mae' in history.history:
                metrics.mae = history.history['val_price_prediction_mae'][best_epoch]

        return metrics

    def _check_convergence(self, history) -> bool:
        """V√©rifie si le mod√®le a converg√©"""
        val_loss = history.history.get('val_loss', [])
        if len(val_loss) < 10:
            return False

        # V√©rifier si la perte de validation s'est stabilis√©e
        recent_loss = val_loss[-5:]
        loss_std = np.std(recent_loss)
        loss_mean = np.mean(recent_loss)

        # Convergence si l'√©cart-type repr√©sente moins de 1% de la moyenne
        return loss_std / (loss_mean + 1e-8) < 0.01

    def _save_training_results(self, results: TrainingResults, history_path: str):
        """Sauvegarde les r√©sultats d'entra√Ænement"""
        try:
            # Sauvegarder l'historique en JSON
            history_data = {
                'training_results': results.to_dict(),
                'config': asdict(self.config),
                'model_config': self.model.get_model_info() if self.model else {}
            }

            with open(history_path, 'w') as f:
                json.dump(history_data, f, indent=2, default=str)

            logger.debug(f"R√©sultats sauvegard√©s: {history_path}")

        except Exception as e:
            logger.error(f"Erreur sauvegarde des r√©sultats: {e}")

    def _update_training_stats(self, results: TrainingResults):
        """Met √† jour les statistiques d'entra√Ænement"""
        self.training_stats['models_trained'] += 1
        self.training_stats['total_training_time'] += results.training_time
        self.training_stats['last_training_date'] = datetime.now(timezone.utc)

        if results.best_metrics.accuracy > self.training_stats['best_accuracy_achieved']:
            self.training_stats['best_accuracy_achieved'] = results.best_metrics.accuracy

        if results.best_metrics.sharpe_ratio > self.training_stats['best_sharpe_achieved']:
            self.training_stats['best_sharpe_achieved'] = results.best_metrics.sharpe_ratio

        if results.convergence_achieved:
            total_models = self.training_stats['models_trained']
            convergence_count = sum(1 for r in self.training_history if r.convergence_achieved) + 1
            self.training_stats['convergence_rate'] = convergence_count / total_models

        # Ajouter aux r√©sultats historiques
        self.training_history.append(results)

    def _plot_training_history(self, history, model_name: str):
        """Cr√©e des graphiques de l'historique d'entra√Ænement"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'Historique d\'entra√Ænement - {model_name}', fontsize=16)

            # Perte
            axes[0, 0].plot(history.history['loss'], label='Train Loss')
            if 'val_loss' in history.history:
                axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')
            axes[0, 0].set_title('Model Loss')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True)

            # Accuracy (si disponible)
            if 'accuracy' in history.history or 'direction_prediction_accuracy' in history.history:
                acc_key = 'accuracy' if 'accuracy' in history.history else 'direction_prediction_accuracy'
                val_acc_key = 'val_' + acc_key

                axes[0, 1].plot(history.history[acc_key], label='Train Accuracy')
                if val_acc_key in history.history:
                    axes[0, 1].plot(history.history[val_acc_key], label='Validation Accuracy')
                axes[0, 1].set_title('Model Accuracy')
                axes[0, 1].set_xlabel('Epoch')
                axes[0, 1].set_ylabel('Accuracy')
                axes[0, 1].legend()
                axes[0, 1].grid(True)

            # MAE (si disponible)
            if 'mae' in history.history or 'price_prediction_mae' in history.history:
                mae_key = 'mae' if 'mae' in history.history else 'price_prediction_mae'
                val_mae_key = 'val_' + mae_key

                axes[1, 0].plot(history.history[mae_key], label='Train MAE')
                if val_mae_key in history.history:
                    axes[1, 0].plot(history.history[val_mae_key], label='Validation MAE')
                axes[1, 0].set_title('Mean Absolute Error')
                axes[1, 0].set_xlabel('Epoch')
                axes[1, 0].set_ylabel('MAE')
                axes[1, 0].legend()
                axes[1, 0].grid(True)

            # Learning rate (si disponible)
            if 'lr' in history.history:
                axes[1, 1].plot(history.history['lr'], label='Learning Rate')
                axes[1, 1].set_title('Learning Rate')
                axes[1, 1].set_xlabel('Epoch')
                axes[1, 1].set_ylabel('Learning Rate')
                axes[1, 1].set_yscale('log')
                axes[1, 1].legend()
                axes[1, 1].grid(True)

            plt.tight_layout()

            # Sauvegarder
            plot_path = os.path.join(self.config.model_dir, "plots", f"{model_name}_history.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()

            logger.debug(f"Graphique sauvegard√©: {plot_path}")

        except Exception as e:
            logger.error(f"Erreur cr√©ation des graphiques: {e}")

    def cross_validate(self, model_config: ModelConfig = None, n_splits: int = 5) -> Dict[str, List[float]]:
        """
        Effectue une validation crois√©e temporelle

        Args:
            model_config: Configuration du mod√®le
            n_splits: Nombre de splits pour la validation crois√©e

        Returns:
            Dictionnaire avec les m√©triques pour chaque fold
        """
        try:
            logger.info(f"üîÑ Validation crois√©e avec {n_splits} folds")

            # Pr√©parer les donn√©es
            X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_data()

            # Combiner train et val pour la CV
            X_full = np.concatenate([X_train, X_val], axis=0)
            y_full = np.concatenate([y_train, y_val], axis=0)

            # Time Series Split
            tscv = TimeSeriesSplit(n_splits=n_splits)

            cv_results = {
                'accuracy': [],
                'mae': [],
                'directional_accuracy': [],
                'val_loss': []
            }

            for fold, (train_idx, val_idx) in enumerate(tscv.split(X_full)):
                logger.info(f"üìä Fold {fold + 1}/{n_splits}")

                # Donn√©es du fold
                X_fold_train, X_fold_val = X_full[train_idx], X_full[val_idx]
                y_fold_train, y_fold_val = y_full[train_idx], y_full[val_idx]

                # Cr√©er et entra√Æner le mod√®le
                model = LSTMModel(model_config or ModelConfig())
                input_shape = (X_fold_train.shape[1], X_fold_train.shape[2])
                keras_model = model.create_model(input_shape)

                # Entra√Ænement rapide pour CV
                history = keras_model.fit(
                    X_fold_train, y_fold_train,
                    validation_data=(X_fold_val, y_fold_val),
                    batch_size=self.config.batch_size,
                    epochs=min(50, self.config.epochs),  # Moins d'√©poques pour la CV
                    verbose=0,
                    callbacks=[
                        keras.callbacks.EarlyStopping(
                            monitor='val_loss',
                            patience=10,
                            restore_best_weights=True
                        )
                    ]
                )

                # Calculer les m√©triques
                metrics = self._calculate_metrics(keras_model, X_fold_val, y_fold_val, f"fold_{fold}")

                cv_results['accuracy'].append(metrics.accuracy)
                cv_results['mae'].append(metrics.mae)
                cv_results['directional_accuracy'].append(metrics.directional_accuracy)
                cv_results['val_loss'].append(min(history.history['val_loss']))

            # Statistiques de CV
            logger.info("üìà R√©sultats de validation crois√©e:")
            for metric, values in cv_results.items():
                mean_val = np.mean(values)
                std_val = np.std(values)
                logger.info(f"   {metric}: {mean_val:.3f} ¬± {std_val:.3f}")

            return cv_results

        except Exception as e:
            logger.error(f"Erreur lors de la validation crois√©e: {e}")
            raise

    def load_model(self, model_path: str) -> bool:
        """
        Charge un mod√®le sauvegard√©

        Args:
            model_path: Chemin vers le mod√®le

        Returns:
            True si chargement r√©ussi
        """
        try:
            if not os.path.exists(model_path):
                logger.error(f"Mod√®le non trouv√©: {model_path}")
                return False

            # Charger le mod√®le Keras
            keras_model = keras.models.load_model(
                model_path,
                custom_objects={'AttentionLayer': AttentionLayer}
            )

            # Cr√©er l'objet LSTMModel
            self.model = LSTMModel()
            self.model.model = keras_model

            # Charger les m√©tadonn√©es si disponibles
            history_path = model_path.replace('.h5', '_history.json')
            if os.path.exists(history_path):
                with open(history_path, 'r') as f:
                    data = json.load(f)
                    if 'model_config' in data:
                        self.model.model_info.update(data['model_config'])

            logger.info(f"‚úÖ Mod√®le charg√©: {model_path}")
            return True

        except Exception as e:
            logger.error(f"Erreur chargement du mod√®le: {e}")
            return False

    def get_training_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques d'entra√Ænement"""
        return self.training_stats.copy()


# Instance globale
model_trainer = ModelTrainer()


# Fonction utilitaire
def train_lstm_model(model_config: ModelConfig = None, training_config: TrainingConfig = None) -> TrainingResults:
    """
    Fonction utilitaire pour entra√Æner un mod√®le LSTM

    Args:
        model_config: Configuration du mod√®le
        training_config: Configuration de l'entra√Ænement

    Returns:
        R√©sultats d'entra√Ænement
    """
    if training_config:
        trainer = ModelTrainer(training_config)
    else:
        trainer = model_trainer

    return trainer.train_model(model_config)


if __name__ == "__main__":
    # Test de l'entra√Æneur
    print("üéì Test de l'entra√Æneur LSTM...")

    try:
        # Configuration de test
        test_model_config = ModelConfig(
            architecture=ModelArchitecture.SIMPLE_LSTM,
            lstm_units=[32],
            dense_units=[16],
            epochs=5,  # Peu d'√©poques pour le test
            batch_size=16,
            lookback_periods=20,
            prediction_horizon=1
        )

        test_training_config = TrainingConfig(
            training_days=7,  # Peu de jours pour le test
            epochs=5,
            batch_size=16,
            patience=3
        )

        trainer = ModelTrainer(test_training_config)

        print(f"‚úÖ Entra√Æneur configur√©")
        print(f"   Mod√®le: {test_model_config.architecture.value}")
        print(f"   √âpoques: {test_training_config.epochs}")
        print(f"   Jours d'entra√Ænement: {test_training_config.training_days}")

        # Test de validation crois√©e (si donn√©es disponibles)
        try:
            cv_results = trainer.cross_validate(test_model_config, n_splits=3)
            print(f"\nüìä Validation crois√©e:")
            for metric, values in cv_results.items():
                print(f"   {metric}: {np.mean(values):.3f} ¬± {np.std(values):.3f}")
        except Exception as e:
            print(f"‚ö†Ô∏è Validation crois√©e √©chou√©e (normal si pas de donn√©es): {e}")

        # Statistiques
        stats = trainer.get_training_stats()
        print(f"\nüìà Statistiques:")
        for key, value in stats.items():
            print(f"   {key}: {value}")

        print("‚úÖ Test de l'entra√Æneur r√©ussi !")

    except Exception as e:
        print(f"‚ùå Erreur lors du test: {e}")
        logger.error(f"Test de l'entra√Æneur √©chou√©: {e}")